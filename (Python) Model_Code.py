# -*- coding: utf-8 -*-
"""GST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-oomrhrJz70cpFggJh4G5JRvaIkC_jmV
"""

# Mount Google Drive to access the dataset stored in the Drive
from google.colab import drive
drive.mount('/content/drive')

# Importing necessary libraries for data analysis and visualization.
# Motivation: These libraries provide essential tools for data handling (pandas), numerical operations (numpy), and visualization (matplotlib, seaborn).
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Loading training data for features and target variable
# Motivation: These datasets contain training samples with their respective features (X) and targets (Y) which we need for supervised learning.
df_Xtrain = pd.read_csv("/content/drive/MyDrive/Datasets/GST/X_Train_Data_Input.csv")
df_Ytrain= pd.read_csv("/content/drive/MyDrive/Datasets/GST/Y_Train_Data_Target.csv")

# Sort both DataFrames by 'ID' in ascending order to ensure alignment before merging.
# Motivation: Sorting by 'ID' ensures that records match correctly between input features and targets during merging.
df_Xtrain= df_Xtrain.sort_values('ID')
df_Ytrain= df_Ytrain.sort_values('ID')

# Merge the DataFrames based on 'ID' to create a complete training set with features and targets.
# Motivation: Merging allows us to bring together the input features and their corresponding target values for model training.
df_train = pd.merge(df_Xtrain, df_Ytrain, on='ID', how='left')

# Reset the index of the merged DataFrame for a clean structure.
# Motivation: Resetting index provides a contiguous sequence of indices, making the DataFrame easier to handle.
df_train.reset_index(drop=True, inplace=True)

# Display the information of the training DataFrame.
# Motivation: This step helps identify data types, number of null values, and the structure of the dataset for further processing.
df_train.info()

# Identify columns with null values and provide the count of unique entries and the number of null values for each.
# Motivation: This helps in understanding the extent of missing data and determining suitable imputation methods.
for column in df_train.columns:
  if df_train[column].isnull().any():
    print(f"Column: {column}")
    print(f"Number of unique entries: {df_train[column].nunique()}")
    print(f"Sum of null values: {df_train[column].isnull().sum()}")
    print("-" * 20)

# Create a correlation matrix excluding the 'ID' column.
# Motivation: Correlation helps to identify relationships between variables, aiding feature selection and multicollinearity checks.
correlation_matrix = df_train.drop('ID', axis=1).corr()

# Visualize the correlation matrix using a heatmap.
# Motivation: The heatmap visually represents correlations, making it easier to spot highly correlated pairs.
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix (Excluding ID)')
plt.show()

# Drop rows where 'Column0' has null values.
# Motivation: If 'Column0' is essential for analysis, rows with missing values in this column might be better excluded rather than imputed.
df_train.dropna(subset=['Column0'], inplace=True)

# Fill null values of specified columns with the median value of that column.
# Motivation: Using the median for imputation is robust against outliers and ensures that the distribution of the data is minimally affected.
columns_to_fill = ['Column5', 'Column6', 'Column9', 'Column14', 'Column15']
for column in columns_to_fill:
  median_val = df_train[column].median()
  df_train[column].fillna(median_val, inplace=True)

# Use ML-based IterativeImputer for columns 'Column3' and 'Column4' which are correlated.
# Motivation: Iterative imputation captures complex relationships between correlated variables, leading to more accurate imputations.
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

columns_to_impute = ['Column3', 'Column4']
df_impute = df_train[columns_to_impute].copy()

# Initialize the IterativeImputer.
imputer = IterativeImputer(max_iter=10, random_state=0)

# Fit and transform the data to fill missing values.
df_imputed = imputer.fit_transform(df_impute)

# Replace the original columns with the imputed values.
df_train[columns_to_impute] = df_imputed

# Use KNN imputation for 'Column8' with k=5.
# Motivation: KNN imputation can be effective when similar patterns exist among the nearest neighbors, helping to predict missing values based on those.
from sklearn.impute import KNNImputer

column_to_impute = ['Column8']
df_knn_impute = df_train[column_to_impute].copy()

# Initialize KNNImputer with 5 neighbors.
imputer = KNNImputer(n_neighbors=5)

# Fit and transform the data for KNN imputation.
df_knn_imputed = imputer.fit_transform(df_knn_impute)

# Replace the original column with the imputed values.
df_train[column_to_impute] = df_knn_imputed

# Check for remaining null values to ensure data is clean.
df_train.isnull().sum()

# Count unique entries in the target column and their occurrences.
# Motivation: Understanding class distribution helps in assessing class imbalance, which is critical for model performance.
unique_targets = df_train['target'].unique()
target_counts = df_train['target'].value_counts()

print("Unique Target Values and their Counts:")
for target in unique_targets:
  print(f"Target: {target}, Count: {target_counts[target]}")

# Load, sort, and merge the test datasets similarly to the training set.
# Motivation: Consistency between training and testing datasets ensures that model evaluation is fair.
df_Xtest = pd.read_csv("/content/drive/MyDrive/Datasets/GST/X_Test_Data_Input.csv")
df_Ytest = pd.read_csv("/content/drive/MyDrive/Datasets/GST/Y_Test_Data_Target.csv")
df_Xtest= df_Xtest.sort_values('ID')
df_Ytest= df_Ytest.sort_values('ID')
df_test = pd.merge(df_Xtest, df_Ytest, on='ID', how='left')
df_test.reset_index(drop=True, inplace=True)

df_test.info()

# Identifying columns with missing values in the test dataset.
# Motivation: Prepares the test data for the same imputation steps used on training data.
for column in df_test.columns:
  if df_test[column].isnull().any():
    print(f"Column: {column}")
    print(f"Number of unique entries: {df_test[column].nunique()}")
    print(f"Sum of null values: {df_test[column].isnull().sum()}")
    print("-" * 20)

# Drop rows with null in 'Column0' in the test set for consistency with training data.
df_test.dropna(subset=['Column0'], inplace=True)

# Fill null values in specified columns with the median.
# Motivation: Consistency in handling missing data across training and test datasets is crucial for model performance.
columns_to_fill = ['Column5', 'Column6', 'Column9', 'Column14', 'Column15']
for column in columns_to_fill:
  median_val = df_test[column].median()
  df_test[column].fillna(median_val, inplace=True)

# Impute columns 3 and 4 using IterativeImputer trained on training data.
# Motivation: Using the training data's pattern ensures that the test data is treated similarly, leading to more reliable results.
df_impute_test = df_test[columns_to_impute].copy()
df_imputed_test = imputer.fit(df_train[columns_to_impute].copy()).transform(df_impute_test)
df_test[columns_to_impute] = df_imputed_test

# Use KNN Imputation for 'Column8' in the test set, using training data's pattern.
# Motivation: Applying the same KNN imputation as done on training data maintains data consistency.
column_to_impute = ['Column8']
df_knn_impute_test = df_test[column_to_impute].copy()
df_knn_imputed_test = imputer.fit(df_train[column_to_impute].copy()).transform(df_knn_impute_test)
df_test[column_to_impute] = df_knn_imputed_test

# Check for remaining null values in the test set to ensure it's clean.
df_test.isnull().sum()

# Get the unique entries and counts in the test set target column.
# Motivation: Understanding the test data distribution is important for evaluating model generalization.
unique_targets_test = df_test['target'].unique()
target_counts_test = df_test['target'].value_counts()

print("Unique Target Values and their Counts in df_test:")
for target in unique_targets_test:
  print(f"Target: {target}, Count: {target_counts_test[target]}")

# Import necessary libraries for model training.
# Motivation: KNN classifier is simple and effective for classification problems, serving as a baseline before more complex models.
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, log_loss, balanced_accuracy_score

# Step 1: Prepare the data by separating features and target.
# Motivation: We need to separate the features and target variable to feed them into the model.
X_train = df_train.drop(columns=['ID', 'target'])
y_train = df_train['target']
X_test = df_test.drop(columns=['ID', 'target'])
y_test = df_test['target']

# Step 2: Normalize the features using StandardScaler.
# Motivation: Normalization helps KNN by ensuring that all features contribute equally to the distance calculations.
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 3: Initialize and train the KNN classifier.
# Motivation: Start with a simple model like KNN to establish a baseline performance.
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# Step 4: Make predictions and evaluate the model.
# Motivation: Metrics like accuracy, recall, precision, and AUC-ROC provide a comprehensive view of model performance.
y_pred = knn.predict(X_test)
y_pred_proba = knn.predict_proba(X_test)[:, 1]

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))
print("ROC AUC Score:", roc_auc_score(y_test, y_pred_proba))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print(f"Log Loss: {logloss:.4f}")
print(f"Balanced Accuracy: {balanced_accuracy:.4f}")

# Install required libraries
# Motivation: CatBoost is a powerful gradient boosting algorithm that handles categorical data efficiently, while 'imbalanced-learn' is needed for handling class imbalances in the dataset.
!pip install catboost
!pip install imbalanced-learn

from catboost import CatBoostClassifier

# Step 4: Train CatBoostClassifier
# Motivation: CatBoost is chosen for its ability to handle categorical features without needing extensive preprocessing, providing better performance with imbalanced datasets.
catboost_model = CatBoostClassifier(
    iterations=1000, 
    learning_rate=0.1, 
    depth=10, 
    eval_metric='F1', 
    verbose=100
)
catboost_model.fit(X_train_scaled, y_train)

# Step 5: Make predictions on the test set
# Motivation: Generate predictions and probability estimates to evaluate the model's performance on unseen data.
y_pred = catboost_model.predict(X_test_scaled)
y_pred_proba = catboost_model.predict_proba(X_test_scaled)[:, 1]  # For AUC-ROC score

# Step 6: Calculate metrics
# Motivation: Use various metrics to evaluate the model's performance comprehensively, ensuring balanced attention to precision, recall, and overall model confidence.
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc_roc = roc_auc_score(y_test, y_pred_proba)
conf_matrix = confusion_matrix(y_test, y_pred)
logloss = log_loss(y_test, y_pred_proba)
balanced_accuracy = balanced_accuracy_score(y_test, y_pred)

# Step 7: Display results
# Motivation: Displaying these metrics helps to understand the strengths and weaknesses of the CatBoost model in terms of different evaluation criteria.
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"AUC-ROC: {auc_roc:.4f}")
print(f"Confusion Matrix:\n{conf_matrix}")
print(f"Log Loss: {logloss:.4f}")
print(f"Balanced Accuracy: {balanced_accuracy:.4f}")

